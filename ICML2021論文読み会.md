# ICML2021論文読み会に参加
* 2021/8/18 19:00~21:00
* #icml2021_reading
* https://line.connpass.com/event/221309/

## Lenient Regret and Good-Action Identification in Gaussian Process Bandits
* 強化学習の話
* Multi-armed bandits
    * 累積リグレット最小化
    * 最も当たる台を見つけたい
* Lenient Regret
    * 古典的な枠組みでは試行回数が膨大であることが問題
    * near-optimalやgood-enoughを求める手法が発展
* Optimal -> suffiinet
* 効率的に解くアルゴリズム
    * Probability of Being Good (PG) -> 少ない試行回数で良い結果が得られた。
    * Expected Improvement Over Good (EG)

## Few-Shot Neural Architecture Search
* Neural Architecture Search (NAS)
* Vanilla NAS: 性能は良いが、GPUで何千時間も計算コストがかかる。
* 改善手法では80GPU時間だが、Vanilla NASより若干性能劣った。

## Oops I Took A Gradient: Scalable Sampling for Discrete Distributions
* Energy-based models are making a comeback
* Gibbs-with-gradients

## A General Framework For Detecting Anomalous Inputs to DNN Classifiers
* Motivation
    * 訓練分布から大きく外れたデータの予測は信頼性が低い
    * 異常入力を検出し、適切な措置をとれるメカニズムを考える。
        * Out-of-distribution
* Proposal
    * DNNの中間層を利用した手法
* Contribution
    * メタアルゴリズム
        * 既存手法のフレームワークで説明できる。
    * 具体的な実現方法
    * 新しい敵対的攻撃方法の提案も含めて評価
* 実験
    * MNIST vs Not-MNISTで異なるデータの検出
* 所感：MLOpsの文脈で使えるか興味があるとのこと。

## Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research
* 強化学習：Rainbow
    * DQNの6手法を全部乗せ手法
* Distributional RLは全部乗せしない方が良い。
* Rainbowを超える手法の実験結果
    * レイヤ数は２～３、Unit数は256以上、バッチサイズは６４以上が良かった。

## Mandoline: Model Evaluation under Distribution Shift
* covariate shift（共変量シフト）
    * 検証と本番で分布が異なる問題
* ノイズのある付加的な情報（スライスと呼ばれる）を利用する方法
* データに関するグルーピングという知識がある場合、それを活かしてシフトを適切にガイド。
* 重点ウェイティングで計算 
    * モンテカルロで近似できる。
* 既存・関連手法
    * KLIEP, targetのＫＬを最小化
    * CBIW: density-ratio trickで密度比推定を２知分類問題に倒す。
    * KMM (Kernel mean matching)
* 既存手法の課題
    * 高次元データでの関数推定はつらい
* 提案手法：MANDOLINE

## Thinking Like Transformers
* RNN -> Transformerに対応する計算モデルはあるか？->作ってみた
* プログラミング言語 RASPを作っちゃいました。
* 言語を通じてTransformerを理解するとは？
    * 解きたいタスクをRASPでプログラムを書く。
    * 想定通りの挙動をしていれば、Transformerを理解したことになる。

## On Disentangled Representations Learned from Correlated Data
* Disentangled Representation
    * 画像を生成因子（性別、髪型など）から完璧に構成することを目指す。
    * できるようになると解釈性が向上する。
* 代表的手法
    * β-VAE：中間層の損失関数にKLダイバージェンスの項を強く導入して、これを最小化する ⇒ ニューロン間が独立になる。
* 生成因子はしばしば相関する。
    * 少量の因子ラベルを用いた後処理や弱教師あり学習で対応。

## Towards Better Robust Generalization with Shift Consistency Regularization
* 分布のずれに対応
    * 敵対的サンプルに対する繁華誤差と経験誤差ギャップに対する不等式評価
    * 正則化の提案
* 微小なノイズで簡単に高精度なモデルを隠す敵対的サンプルを作成可能
    * パンダ＋微小なノイズ＝テナガザル100%
* 微小なノイズに対しても頑健性を得たい。
    * どういう最適化問題を解けば良いか
* xの近くなので突然あほになったらペナルティ
* Adversarial Trainingではlabel leakingを起こしてしまう。
* Cleanなサンプルの近傍において、入力空間で定義されるＷＤ。
* 敵対的サンプルに対する訓練データとテストデータの性能差を小さくしたい。
* DNNなどの分類境界が柔軟すぎることが敵対的サンプルが悪さする主要因と考えられる。

## DeepWalking Backwards: From Embeddings Back to Graphs
* SANSAN
    * 名刺管理会社 ⇒ グラフネットワーク ⇒ 今回の論文を選んだ
* node embeddingが優れた結果をもたらす理由をより理解するため、どのような情報を保持する埋め込みであるのかを明らかにしたい
* Node embedding
    * 離散的なグラフのノードを、低次元な蜜ベクトルで表現する技術
    * 様々な後段に有用
* DeepWalk
    * word2vecをグラフに適用したアルゴリズム
    * ノードの系列をランダムにWalkする。
    * node2vec, LINE, NetMF
* NetMF
    * ノード共起の自己相関情報量（PPMI）行列を低ランク近似したもの
* NetMF (DeepWalk)の逆変換（embedding inversion）を２つ提案
    * Analytical approach
    * Optimization approach -> こちらの方が実験結果は良かった